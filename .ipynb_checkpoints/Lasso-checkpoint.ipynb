{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games = pd.read_pickle('game_data/games_2017.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Shuffle DataFrame'''\n",
    "games = games.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't need: Date, Ws, Tm, OPTm, ID, count, or matchup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xy = games[['W', 'Wp', 'ppg', 'pApg', 'FGp', '3Pp', 'FTp', 'ORBpg', 'RBpg', \n",
    "            'ASTpg', 'STLpg', 'BLKpg', 'TOpg', 'PFpg', 'sos', 'OPppg', \n",
    "            'OPpApg', 'OPFGp', 'OP3Pp', 'OPFTp', 'OPORBpg', 'OPRBpg', \n",
    "            'OPASTpg', 'OPSTLpg', 'OPBLKpg', 'OPTOpg', 'OPPFpg', 'OPsos']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up features and targets\n",
    "X = Xy.iloc[:, 1:]\n",
    "y = Xy.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train test split'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Standardize Data'''\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Fit model on training data'''\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.10328596, -0.09382236, -0.36350849,  0.25248479,  0.13498798,\n",
       "         0.19063195, -0.04265419,  0.28548057, -0.0397107 ,  0.16614536,\n",
       "        -0.09771024,  0.08546571,  0.0453361 ,  0.39383866, -1.23803575,\n",
       "         1.22169319, -0.29871273,  0.0017944 ,  0.10291181, -0.06329962,\n",
       "         0.27443422,  0.08250475,  0.04180119, -0.28358488,  0.04792718,\n",
       "         0.0345399 , -0.42276027]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''predict on testing data'''\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80 (% predicted correctly)\n",
      "Precision: 0.83 (predicted positives % correct)\n",
      "Ave. Precision: 0.76 (predicted positives % correct)\n",
      "Recall: 0.78 (% of positives predicted correctly)\n"
     ]
    }
   ],
   "source": [
    "lg_accuracy = metrics.accuracy_score(y_test, y_hat)\n",
    "lg_ave_precision = metrics.average_precision_score(y_test, y_hat)\n",
    "lg_f1 = metrics.f1_score(y_test, y_hat)\n",
    "lg_log_loss = metrics.log_loss(y_test, y_hat)\n",
    "lg_precision = metrics.precision_score(y_test, y_hat)\n",
    "lg_recall = metrics.recall_score(y_test, y_hat)\n",
    "# roc_auc_score = metrics.roc_auc_score(y_test, y_hat)\n",
    "print('Accuracy: {:.2f} (% predicted correctly)'.format(lg_accuracy))\n",
    "print('Precision: {:.2f} (predicted positives % correct)'.format(lg_precision))\n",
    "print('Ave. Precision: {:.2f} (predicted positives % correct)'.format(lg_ave_precision))\n",
    "print('Recall: {:.2f} (% of positives predicted correctly)'.format(lg_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "- Standardize!\n",
    "    - right after train test split\n",
    "    - before cross-validation\n",
    "- Cross-validation (KFolds)\n",
    "- Regularization\n",
    "    - Ridge\n",
    "    - Lasso\n",
    "    - Elastinet\n",
    "        - optimize alpha parameters for each\n",
    "- ROC threshold optimization\n",
    "\n",
    "\n",
    "Future:\n",
    "- Train on data from multiple years.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'alpha': .5}\n",
    "ridge = Ridge(**params)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15781015, -0.01158434, -0.04866317,  0.02601662,  0.02324908,\n",
       "        0.02413204, -0.00413427,  0.03685575, -0.00917176,  0.01991362,\n",
       "       -0.02240153,  0.01802902,  0.00341365,  0.03877407, -0.18153405,\n",
       "        0.17601333, -0.03406445, -0.00632396,  0.01589088, -0.00474978,\n",
       "        0.04583388,  0.01584016,  0.00688521, -0.03285257, -0.00525019,\n",
       "        0.00533618, -0.04925231])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_predict = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Accuracy: 0.80 (% predicted correctly)\n",
      "Ridge Precision: 0.83 (predicted positives % correct)\n",
      "Ridge Ave. Precision: 0.76 (predicted positives % correct)\n",
      "Ridge Recall: 0.78 (% of positives predicted correctly)\n"
     ]
    }
   ],
   "source": [
    "ridge_accuracy = metrics.accuracy_score(y_test, y_hat)\n",
    "ridge_ave_precision = metrics.average_precision_score(y_test, y_hat)\n",
    "ridge_f1 = metrics.f1_score(y_test, y_hat)\n",
    "ridge_log_loss = metrics.log_loss(y_test, y_hat)\n",
    "ridge_precision = metrics.precision_score(y_test, y_hat)\n",
    "ridge_recall = metrics.recall_score(y_test, y_hat)\n",
    "# roc_auc_score = metrics.roc_auc_score(y_test, y_hat)\n",
    "print('Ridge Accuracy: {:.2f} (% predicted correctly)'.format(ridge_accuracy))\n",
    "print('Ridge Precision: {:.2f} (predicted positives % correct)'.format(ridge_precision))\n",
    "print('Ridge Ave. Precision: {:.2f} (predicted positives % correct)'.format(ridge_ave_precision))\n",
    "print('Ridge Recall: {:.2f} (% of positives predicted correctly)'.format(ridge_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'alpha': .5}\n",
    "lasso = Lasso(**params)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.,\n",
       "        0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_predict = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Accuracy: 0.80 (% predicted correctly)\n",
      "Lasso Precision: 0.83 (predicted positives % correct)\n",
      "Lasso Ave. Precision: 0.76 (predicted positives % correct)\n",
      "Lasso Recall: 0.78 (% of positives predicted correctly)\n"
     ]
    }
   ],
   "source": [
    "lasso_accuracy = metrics.accuracy_score(y_test, y_hat)\n",
    "lasso_ave_precision = metrics.average_precision_score(y_test, y_hat)\n",
    "lasso_f1 = metrics.f1_score(y_test, y_hat)\n",
    "lasso_log_loss = metrics.log_loss(y_test, y_hat)\n",
    "lasso_precision = metrics.precision_score(y_test, y_hat)\n",
    "lasso_recall = metrics.recall_score(y_test, y_hat)\n",
    "# roc_auc_score = metrics.roc_auc_score(y_test, y_hat)\n",
    "print('Lasso Accuracy: {:.2f} (% predicted correctly)'.format(ridge_accuracy))\n",
    "print('Lasso Precision: {:.2f} (predicted positives % correct)'.format(ridge_precision))\n",
    "print('Lasso Ave. Precision: {:.2f} (predicted positives % correct)'.format(ridge_ave_precision))\n",
    "print('Lasso Recall: {:.2f} (% of positives predicted correctly)'.format(ridge_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'alpha': .5}\n",
    "enet = ElasticNet(**params)\n",
    "enet.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.,\n",
       "        0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_predict = enet.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Accuracy: 0.80 (% predicted correctly)\n",
      "ElasticNet Precision: 0.83 (predicted positives % correct)\n",
      "ElasticNet Ave. Precision: 0.76 (predicted positives % correct)\n",
      "ElasticNet Recall: 0.78 (% of positives predicted correctly)\n"
     ]
    }
   ],
   "source": [
    "enet_accuracy = metrics.accuracy_score(y_test, y_hat)\n",
    "enet_ave_precision = metrics.average_precision_score(y_test, y_hat)\n",
    "enet_f1 = metrics.f1_score(y_test, y_hat)\n",
    "enet_log_loss = metrics.log_loss(y_test, y_hat)\n",
    "enet_precision = metrics.precision_score(y_test, y_hat)\n",
    "enet_recall = metrics.recall_score(y_test, y_hat)\n",
    "# roc_auc_score = metrics.roc_auc_score(y_test, y_hat)\n",
    "print('ElasticNet Accuracy: {:.2f} (% predicted correctly)'.format(enet_accuracy))\n",
    "print('ElasticNet Precision: {:.2f} (predicted positives % correct)'.format(enet_precision))\n",
    "print('ElasticNet Ave. Precision: {:.2f} (predicted positives % correct)'.format(enet_ave_precision))\n",
    "print('ElasticNet Recall: {:.2f} (% of positives predicted correctly)'.format(enet_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regularizaton models are just regularizing back down to basic logistic regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize_X(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    # Use X_train to compute mean and standard deviation\n",
    "    scaler.fit(X_train)\n",
    "    # Standardize X_train\n",
    "    X_train_standardized = scaler.transform(X_train)\n",
    "    # Standardize X_test\n",
    "    X_test_standardized = scaler.transform(X_test)\n",
    "    return X_train_standardized, X_test_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(X, y, model, n_folds, errortype='RMSE', random_seed=154):\n",
    "    \"\"\"Estimate the in- and out-of-sample error of a model using cross\n",
    "    validation.\n",
    "    \n",
    "    Requirements\n",
    "    ----------\n",
    "    class XyScaler\n",
    "    function rmsle\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    " \n",
    "    X: np.array\n",
    "      Matrix of predictors, standardized\n",
    " \n",
    "    y: np.array\n",
    "      Target array, standardized\n",
    " \n",
    "    model: sklearn model object.\n",
    "      The estimator to fit.  Must have fit and predict methods.\n",
    " \n",
    "    n_folds: int\n",
    "      The number of folds in the cross validation.\n",
    "    \n",
    "    errortype: string\n",
    "      either 'RMSE' or 'RMSLE'\n",
    " \n",
    "    random_seed: int\n",
    "      A seed for the random number generator, for repeatability.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    " \n",
    "    errors, cfs_best: tuple of arrays\n",
    "      errors = (train errors, test errors) for each fold of cross-validation\n",
    "      cfs-best = coefficients selected from minimum test error\n",
    "    \"\"\"\n",
    "    kf = KFold(n_folds)\n",
    "    errorlist = []\n",
    "    cfs = []\n",
    "    for k, (train_index,test_index) in enumerate(kf.split(X)):\n",
    "        # define variables\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        # fit model\n",
    "        model.fit(X_train,y_train)\n",
    "        y_hat_train = model.predict(X_train)\n",
    "        y_hat_test = model.predict(X_test) \n",
    "        # evaluate model\n",
    "        if errortype == 'RMSE':\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_hat_train,y_train))\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_hat_test,y_test))\n",
    "            errorlist.append((rmse_train, rmse_test)) # tuple output\n",
    "        elif errortype == 'RMSLE':\n",
    "            rmsle_train = rmsle(y_train, y_hat_train)\n",
    "            rmsle_test = rmsle(y_test, y_hat_test)\n",
    "            errorlist.append((rmsle_train, rmsle_test)) # tuple output\n",
    "        # store coefficients\n",
    "        cfs.append (model.coef_)\n",
    "    # select best coefficients \n",
    "    errors = np.asarray(errorlist)\n",
    "    idx_min_test_error = errors[:,1].argmin()   \n",
    "    cfs_best = cfs[idx_min_test_error]\n",
    "    \n",
    "    return(errors, cfs_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_at_various_alphas(X, y, model, alphas, n_folds=10, errortype='RMSE', **kwargs):\n",
    "    \"\"\"Train a regularized regression model using cross validation at various\n",
    "    values of alpha.\n",
    "    \n",
    "    requirements\n",
    "    ----------\n",
    "    class XyScaler\n",
    "    function rmsle\n",
    "    function cross_val\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    " \n",
    "    X: np.array\n",
    "      Matrix of predictors, standardized\n",
    " \n",
    "    y: np.array\n",
    "      Target array, standardized\n",
    " \n",
    "    model: name of sklearn model class\n",
    "      A class in sklearn that can be used to create a regularized regression\n",
    "      object.  Options are `Ridge` and `Lasso`.\n",
    " \n",
    "    alphas: numpy array\n",
    "      An array of regularization parameters.\n",
    " \n",
    "    n_folds: int\n",
    "      Number of cross validation folds.\n",
    "    \n",
    "    errortype: string\n",
    "      either 'RMSE' or 'RMSLE'\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    " \n",
    "    cv_errors_train, cv_errors_test: tuple of DataFrame\n",
    "      DataFrames containing the training and testing errors for each value of\n",
    "      alpha and each cross validation fold.  Each row represents a CV fold, and\n",
    "      each column a value of alpha.\n",
    "      \n",
    "      Dataframe containing coefficients for each parameter for each alpha\n",
    "    \"\"\"\n",
    "    cv_errors_train = pd.DataFrame(np.empty(shape=(n_folds, len(alphas))),\n",
    "                                     columns=alphas)\n",
    "    cv_errors_test = pd.DataFrame(np.empty(shape=(n_folds, len(alphas))),\n",
    "                                        columns=alphas)\n",
    "    coefs_df = pd.DataFrame(np.empty(shape=(n_folds, len(alphas))),\n",
    "                                        columns=alphas)\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        errors, coefs = cross_val(X, y, model(alpha), n_folds, errortype='RMSE', random_seed=154)\n",
    "        cv_errors_train.iloc[:,idx] = errors[:,0]\n",
    "        cv_errors_test.iloc[:,idx] = errors[:,1]\n",
    "        #coefs_df.iloc[:,idx] = coefs\n",
    "    return(cv_errors_train, cv_errors_test)#, coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
